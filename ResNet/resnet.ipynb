{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Neuron With Backpropagation\n",
    "## Forward Pass\n",
    "$$ z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b$$\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "Compute the output using the dot product of weights and features.\n",
    "\n",
    "## Loss \n",
    "The Mean Squared Error quantifies the error between the neuron's predictions and the actual labels:\n",
    "$$ \n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^n(\\sigma(z_i) - y_i)^2$$\n",
    "\n",
    "## Backward Pass\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial MSE}{\\partial w_j} &= \\frac{2}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) \\sigma'(z_i)x_{ij} \\\\\n",
    " \n",
    "\\frac{\\partial MSE}{\\partial b} &= \\frac{2}{n} \\sum_{i=1}^n (\\sigma(z_i) - y_i) \\sigma'(z_i)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Update the Parameters\n",
    "$$\n",
    "\\begin{align}\n",
    "w_j &= w_j - \\alpha \\frac{\\partial MSE}{\\partial w_j} \\\\\n",
    " \n",
    "b &= b - \\alpha \\frac{\\partial MSE}{\\partial b}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_neuron(features: np.ndarray, labels: np.ndarray, \n",
    "                initial_weights: np.ndarray, initial_bias: float, \n",
    "                learning_rate: float, epochs: int) -> (np.ndarray, float, list[float]):\n",
    "    \n",
    "    updated_weights = initial_weights.copy()\n",
    "    updated_bias = initial_bias\n",
    "    mse_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        grad_weights = np.zeros_like(updated_weights)\n",
    "        grad_bias = 0\n",
    "        preds = []\n",
    "\n",
    "        for x, y_true in zip(features, labels):\n",
    "            z = np.dot(updated_weights, x) + updated_bias\n",
    "            y_pred = 1 / (1 + np.exp(-z))\n",
    "            preds.append(y_pred)\n",
    "            \n",
    "            loss = (y_pred - y_true) ** 2\n",
    "            total_loss += loss\n",
    "\n",
    "            dloss = 2 * (y_pred - y_true)\n",
    "            dsig = y_pred * (1 - y_pred)\n",
    "\n",
    "            grad = dloss * dsig\n",
    "            grad_weights += grad * x\n",
    "            grad_bias += grad\n",
    "        \n",
    "        m = len(features)\n",
    "        preds = np.array(preds)\n",
    "        mse = np.mean((preds - labels) ** 2)\n",
    "        mse_values.append(round(mse, 4))\n",
    "\n",
    "        updated_weights -= learning_rate * (grad_weights / m)\n",
    "        updated_bias -= learning_rate * (grad_bias / m)\n",
    "\n",
    "    return updated_weights, updated_bias, mse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Weights: [ 0.1035744  -0.14254396]\n",
      "Updated Bias: -0.016719880375037202\n",
      "MSE Values: [np.float64(0.3033), np.float64(0.2942)]\n"
     ]
    }
   ],
   "source": [
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "updated_weights, updated_bias, mse_values = train_neuron(\n",
    "        np.array(features), np.array(labels),\n",
    "        np.array(initial_weights), initial_bias,\n",
    "        learning_rate, epochs\n",
    ")\n",
    "\n",
    "print(\"Updated Weights:\", updated_weights)\n",
    "print(\"Updated Bias:\", updated_bias)\n",
    "print(\"MSE Values:\", mse_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Convolutional 2D Layer\n",
    "1. `input_matrix` the input data, for an image this is each pixel\n",
    "2. `kernel` the filter kernel \n",
    "3. `padding` the extra space in the input to allow the kernel to fit.\n",
    "4. `stride` the number of steps the kernel moves across the input.\n",
    "\n",
    "A Convolution is an element-wise multiplication between the kernel and the input window, followed by a sum of the results, stored in the output matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):\n",
    "    input_height, input_width = input_matrix.shape\n",
    "    kernel_height, kernel_width = kernel.shape\n",
    "\n",
    "    input_matrix = np.pad(input_matrix, pad_width=padding)\n",
    "\n",
    "    input_height_pad, input_width_pad = input_matrix.shape\n",
    "\n",
    "    OH = ((input_height_pad - kernel_height) // stride) + 1\n",
    "    OW = ((input_width_pad - kernel_width) // stride) + 1\n",
    "\n",
    "    output_matrix = np.zeros((OW, OH))\n",
    "\n",
    "    for i in range(0, OH):\n",
    "        for j in range(0, OW):\n",
    "            p = i * stride\n",
    "            q = j * stride\n",
    "            window = input_matrix[p:p+kernel_height, q:q+kernel_width]\n",
    "        \n",
    "            output_matrix[i,j] = np.sum(window * kernel)\n",
    "            \n",
    "    return output_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1. -4.]\n",
      " [ 9.  7. -4.]\n",
      " [ 0. 14. 16.]]\n"
     ]
    }
   ],
   "source": [
    "input_matrix = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [5, 6, 7, 8],\n",
    "    [9, 10, 11, 12],\n",
    "    [13, 14, 15, 16]\n",
    "])\n",
    "\n",
    "kernel = np.array([\n",
    "    [1, 0],\n",
    "    [-1, 1]\n",
    "])\n",
    "\n",
    "padding = 1\n",
    "stride = 2\n",
    "\n",
    "output = simple_conv2d(input_matrix, kernel, padding, stride)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU\n",
    "The ReLU (Rectified Linear Unit) activation function is widely used in neural networks, particularly in hidden layers of deep learning models. It maps any real-valued number to the non-negative range $[0,\\infin)$, which helps introduce non-linearity into the model while maintaining computational efficiency.\n",
    "\n",
    "$$f(z) = max(0, z)$$\n",
    "\n",
    "- It has an L shaped curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(z: np.ndarray) -> np.ndarray:\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2]\n",
      " [3 0]]\n"
     ]
    }
   ],
   "source": [
    "out = ReLU(np.array([[-1, 2], [3, -4]]))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Residual Connections\n",
    "The main bit of ResNet, in a traditional network the output is a direct transformation of the input, $H(x)$. In a residual block, the network learns the residual $F(x) = H(x) - x$ and the output becomes:\n",
    "$$ y = F(x) = x$$\n",
    "\n",
    "A residual connection has two weight layers and an activation between them.\n",
    "\n",
    "- It uses ReLU activation \n",
    "\n",
    "## Why?\n",
    "- ease of learning \n",
    "- gradient flow: allows gradients to flow directly through the addition, removing vanishing gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x: np.ndarray, w1: np.ndarray, w2: np.ndarray) -> np.ndarray:\n",
    "    first_layer = w1 @ x \n",
    "    relu_1 = ReLU(first_layer)\n",
    "\n",
    "    second_layer = w2 @ relu_1\n",
    "\n",
    "    res = second_layer + x\n",
    "\n",
    "    return ReLU(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Block Output: [1.5 3. ]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 2.0])\n",
    "w1 = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "w2 = np.array([[0.5, 0.0], [0.0, 0.5]])\n",
    "\n",
    "output = residual_block(x, w1, w2)\n",
    "print(\"Residual Block Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Average Pooling (GAP) is a Pooling Operation used in CNNs to reduce the spatial dimensions of feature maps.\n",
    "\n",
    "For a 3D input tensor:\n",
    "- $H$ is the height\n",
    "- $W$ is the width\n",
    "- $C$ is the number of channels (feature maps)\n",
    "\n",
    "$$\n",
    "GAP(x)_c = \\frac{1}{H \\times W} \\sum_{i=1}^H \\sum_{j=1}^W x_{i,j,c}\n",
    "$$\n",
    "It returns a 1D vector of shape $(C,)$ where each element is the average of all values in that corresponding feature map.\n",
    "\n",
    "Essentially:\n",
    "```py\n",
    "np.mean(x, axis=(0, 1))\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "- Parameter Reduction: By replacing fully connected layers with GAP, the number of parameters is significantly reduced, which helps in preventing overfitting.\n",
    "- Spatial Invariance: GAP captures the global information from each feature map, making the model more robust to spatial translations.\n",
    "- Simplicity: It is a straightforward operation that doesn't require tuning hyperparameters like pooling window size or stride.\n",
    "\n",
    "Global Average Pooling is a key component in architectures like ResNet, where it is used before the final classification layer. It allows the network to handle inputs of varying sizes, as the output depends only on the number of channels, not the spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_avg_pool(x: np.ndarray) -> np.ndarray:\n",
    "    width, height, channels = x.shape\n",
    "    output = np.zeros(channels)\n",
    "\n",
    "    for c in range(channels):\n",
    "        channel_sum = 0\n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                channel_sum += x[i, j, c]\n",
    "            output[c] = channel_sum / (height * width)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.5 6.5 7.5]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\n",
    "print(global_avg_pool(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalisation for BCHW\n",
    "- Batch Normalisation (BN) helps accelerate training of networks.\n",
    "- This makes the learning process more stable and speeds up convergence, also introduces regularisation.\n",
    "\n",
    "BN works by reducing internal covariate shift, which happens when the distribution of inputs to a layer changes during training. \n",
    "\n",
    "BN is done via the following steps:\n",
    "1. Compute the Mean and Variance: for each mini-batch.\n",
    "2. Normalise the inputs using mean and variance.\n",
    "3. Apply Scale and Shift: after norm apply a learned gamma and shift to restore the model's ability to represent data in the original distribution.\n",
    "4. Training and Inference\n",
    "\n",
    "For an input tensor with shape BCHW (Batch Size, Channels, Height, Width)\n",
    "\n",
    "1. Mean and Variance\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu_c &= \\frac{1}{B \\cdot H \\cdot W} \\sum_{i=1}^B \\sum_{h=1}^H \\sum_{w=1}^W x_{i,c,h,w} \\\\\n",
    "\n",
    "\\sigma^2_c &= \\frac{1}{B \\cdot H \\cdot W} \\sum_{i=1}^B \\sum_{h=1}^H \\sum_{w=1}^W (x_{i,c,h,w} - \\mu_c)^2 \n",
    "\\end{align}\n",
    "$$\n",
    "Where $x_{i,c,h,w}$ is the input activation at batch index $i$, channel $c$, height $h$, and width $w$.\n",
    "\n",
    "2. Normalisation \n",
    "$$\n",
    "\\hat{x}_{i,c,h,w} = \\frac{x_{i,c,h,w} - \\mu_c}{\\sqrt{\\sigma^2_c} + \\epsilon}\n",
    "$$\n",
    "Use use $\\epsilon$ for numerical stability (avoiding division by 0)\n",
    "\n",
    "3. Scale and Shift\n",
    "\n",
    "Then we apply a scale ($\\gamma_c$) and a shift ($\\beta_c$), to adjust the distribution of features.\n",
    "\n",
    "$$\n",
    "y_{i,c,h,w} = \\gamma_c \\hat{x}_{i,c,h,w} + \\beta_c\n",
    "$$\n",
    "\n",
    "### Key Points \n",
    "- Channel-wise Normalization: Batch Normalization normalizes the activations independently for each channel (C) because different channels in convolutional layers often have different distributions and should be treated separately.\n",
    "- Improved gradient flow by reducing internal covariate shift, allowing faster and more reliable convergence.\n",
    "- Introduces noise and acts as regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(X: np.ndarray, gamma: np.ndarray, beta: np.ndarray, epsilon: float = 1e-5) -> np.ndarray:\n",
    "    mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n",
    "    var = np.var(X, axis=(0, 2, 3), keepdims=True)\n",
    "\n",
    "    x_norm = (X - mean) / np.sqrt(var + epsilon)\n",
    "\n",
    "    out = gamma * x_norm + beta\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.42859934 -0.51776438]\n",
      "   [ 0.65360963  1.95820707]]\n",
      "\n",
      "  [[ 0.02353721  0.02355215]\n",
      "   [ 1.67355207  0.93490043]]]\n",
      "\n",
      "\n",
      " [[[-1.01139563  0.49692747]\n",
      "   [-1.00236882 -1.00581468]]\n",
      "\n",
      "  [[ 0.45676349 -1.50433085]\n",
      "   [-1.33293647 -0.27503802]]]]\n"
     ]
    }
   ],
   "source": [
    "B, C, H, W = 2, 2, 2, 2\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(B, C, H, W)\n",
    "gamma = np.ones(C).reshape(1, C, 1, 1)\n",
    "beta = np.zeros(C).reshape(1, C, 1, 1)\n",
    "actual_output = batch_normalization(X, gamma, beta)\n",
    "print(actual_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
