{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a06ff847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b269d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): # RoPE\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the array\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in the array\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register pe as a buffer so it is not a model parameter but still part of the model's state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "    \n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def create_padding_mask(seq, pad_idx=0):\n",
    "    # Create a mask for padding tokens: 1 for non-pad, 0 for pad\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    # Create a triangular mask to hide future tokens\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1).bool()\n",
    "    return ~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0262da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, sentence, src_vocab, tgt_idx2word, device, max_len=100):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize and convert to indices\n",
    "    tokens = sentence.split()\n",
    "    src_indices = [src_vocab.get(word, src_vocab['']) for word in tokens]\n",
    "    src_indices = [src_vocab['']] + src_indices + [src_vocab['']]\n",
    "    \n",
    "    # Pad source sequence\n",
    "    src_indices = src_indices + [src_vocab['']] * (max_len - len(src_indices))\n",
    "    src_indices = src_indices[:max_len]\n",
    "    \n",
    "    # Convert to tensor\n",
    "    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Create mask\n",
    "    src_mask = create_padding_mask(src_tensor)\n",
    "    \n",
    "    # Get encoder output\n",
    "    enc_output = model.encode(src_tensor, src_mask)\n",
    "    \n",
    "    # Initialize decoder input with  token\n",
    "    dec_input = torch.tensor([[src_vocab['']]], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Generate translation\n",
    "    output_indices = []\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        # Create mask for decoder input\n",
    "        tgt_mask = create_look_ahead_mask(dec_input.size(1)).to(device)\n",
    "        \n",
    "        # Get decoder output\n",
    "        dec_output = model.decode(dec_input, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        # Get predicted token\n",
    "        pred = model.linear(dec_output[:, -1])\n",
    "        pred_idx = pred.argmax(dim=-1).item()\n",
    "        \n",
    "        # Add predicted token to output\n",
    "        output_indices.append(pred_idx)\n",
    "        \n",
    "        # Check if end of sequence\n",
    "        if pred_idx == src_vocab['']:\n",
    "            break\n",
    "        \n",
    "        # Update decoder input\n",
    "        dec_input = torch.cat([dec_input, torch.tensor([[pred_idx]], dtype=torch.long).to(device)], dim=1)\n",
    "    \n",
    "    # Convert indices to words\n",
    "    output_words = [tgt_idx2word.get(idx, '') for idx in output_indices]\n",
    "    \n",
    "    # Remove special tokens\n",
    "    output_words = [word for word in output_words if word not in ['', '', '']]\n",
    "    \n",
    "    return ' '.join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b2b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        # Linear Projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Calculate attention scores\n",
    "        # Q K V Shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        scores = Q @ K.transpose(-2, -1) / (self.d_k ** 0.5)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Softmax to get attention weights\n",
    "        attn_weights = scores.softmax(dim=-1)\n",
    "\n",
    "        # Apply attention weights to V\n",
    "        output = attn_weights @ V\n",
    "        \n",
    "        return output \n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Linear projections\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask=attn_mask)\n",
    "\n",
    "        # Concatenate heads and apply final linear projection\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d886857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, max_len=100):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert source sentence to indices\n",
    "        src_indices = [self.src_vocab.get(word, self.src_vocab['']) for word in self.src_sentences[idx].split()]\n",
    "        src_indices = [self.src_vocab['']] + src_indices + [self.src_vocab['']]\n",
    "        \n",
    "        # Convert target sentence to indices\n",
    "        tgt_indices = [self.tgt_vocab.get(word, self.tgt_vocab['']) for word in self.tgt_sentences[idx].split()]\n",
    "        tgt_indices = [self.tgt_vocab['']] + tgt_indices + [self.tgt_vocab['']]\n",
    "        \n",
    "        # Pad sequences\n",
    "        src_indices = src_indices[:self.max_len]\n",
    "        tgt_indices = tgt_indices[:self.max_len]\n",
    "        \n",
    "        src_indices = src_indices + [self.src_vocab['']] * (self.max_len - len(src_indices))\n",
    "        tgt_indices = tgt_indices + [self.tgt_vocab['']] * (self.max_len - len(tgt_indices))\n",
    "        \n",
    "        return {\n",
    "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
    "            'tgt': torch.tensor(tgt_indices[:-1], dtype=torch.long), # Input to decoder\n",
    "            'tgt_y': torch.tensor(tgt_indices[1:], dtype=torch.long) # Expected output\n",
    "        }\n",
    "    \n",
    "def create_toy_dataset():\n",
    "    # Simple English to French translation pairs\n",
    "    eng_sentences = [\n",
    "        'hello how are you',\n",
    "        'i am fine thank you',\n",
    "        'what is your name',\n",
    "        'my name is john',\n",
    "        'where do you live',\n",
    "        'i live in new york',\n",
    "        'i love programming',\n",
    "        'this is a test',\n",
    "        'please translate this',\n",
    "        'thank you very much'\n",
    "    ]\n",
    "    \n",
    "    fr_sentences = [\n",
    "        'bonjour comment vas tu',\n",
    "        'je vais bien merci',\n",
    "        'quel est ton nom',\n",
    "        'je m appelle john',\n",
    "        'où habites tu',\n",
    "        'j habite à new york',\n",
    "        'j aime programmer',\n",
    "        'c est un test',\n",
    "        's il te plaît traduis cela',\n",
    "        'merci beaucoup'\n",
    "    ]\n",
    "    \n",
    "    # Create vocabularies\n",
    "    src_vocab = {'': 0, '': 1, '': 2, '': 3}\n",
    "    tgt_vocab = {'': 0, '': 1, '': 2, '': 3}\n",
    "    \n",
    "    # Add words to vocabularies\n",
    "    i = 4\n",
    "    for sent in eng_sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in src_vocab:\n",
    "                src_vocab[word] = i\n",
    "                i += 1\n",
    "    \n",
    "    i = 4\n",
    "    for sent in fr_sentences:\n",
    "        for word in sent.split():\n",
    "            if word not in tgt_vocab:\n",
    "                tgt_vocab[word] = i\n",
    "                i += 1\n",
    "    \n",
    "    # Create reverse vocabularies for decoding\n",
    "    src_idx2word = {idx: word for word, idx in src_vocab.items()}\n",
    "    tgt_idx2word = {idx: word for word, idx in tgt_vocab.items()}\n",
    "    \n",
    "    return eng_sentences, fr_sentences, src_vocab, tgt_vocab, src_idx2word, tgt_idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "898af4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):  \n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mlp = MLP(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Self-attention with mask + residual connection + layer normalization\n",
    "        self_attn_output = self.self_attn(x, x, x, attn_mask=tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(self_attn_output))\n",
    "\n",
    "        # Cross-attention with mask + residual connection + layer normalization\n",
    "        cross_attn_output = self.cross_attn(x, enc_output, enc_output, attn_mask=src_mask)\n",
    "        x = self.norm2(x + self.dropout(cross_attn_output))\n",
    "\n",
    "        # Feed-forward network with residual connection and layer normalization\n",
    "        ff_output = self.mlp(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # apply embedding and positional encoding\n",
    "        x = self.embedding(x) * (self.embedding.embedding_dim ** 0.5)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a733d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mlp = MLP(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # Add residual connection\n",
    "\n",
    "        # Feed-forward network with residual connection and layer normalization\n",
    "        ff_output = self.mlp(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Apply embedding and positional encoding\n",
    "        x = self.embedding(x) * (self.embedding.embedding_dim ** 0.5)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dda1900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8,\n",
    "                  d_ff=2048, num_layers=6, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(tgt_vocab_size, d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Encode source sequence\n",
    "        enc_output = self.encoder(src, mask=src_mask)\n",
    "        \n",
    "        # Decode target sequence\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "\n",
    "        # Final linear layer to project to target vocabulary size\n",
    "        output = self.linear(dec_output)\n",
    "        return output\n",
    "    \n",
    "    def encode(self, src, src_mask=None):\n",
    "        return self.encoder(src, mask=src_mask)\n",
    "    \n",
    "    def decode(self, tgt, enc_output, src_mask=None, tgt_mask=None):\n",
    "        return self.decoder(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d8cae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        src = batch['src'].to(device)\n",
    "        tgt = batch['tgt'].to(device)\n",
    "        tgt_y = batch['tgt_y'].to(device)\n",
    "        \n",
    "        # Create masks\n",
    "        src_mask = create_padding_mask(src)\n",
    "        tgt_mask = create_padding_mask(tgt) & create_look_ahead_mask(tgt.size(1)).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt, src_mask, tgt_mask)\n",
    "        \n",
    "        # Reshape output and target for loss calculation\n",
    "        output = output.view(-1, output.size(-1))\n",
    "        tgt_y = tgt_y.view(-1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, tgt_y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate_transformer(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            src = batch['src'].to(device)\n",
    "            tgt = batch['tgt'].to(device)\n",
    "            tgt_y = batch['tgt_y'].to(device)\n",
    "            \n",
    "            # Create masks\n",
    "            src_mask = create_padding_mask(src)\n",
    "            tgt_mask = create_padding_mask(tgt) & create_look_ahead_mask(tgt.size(1)).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            \n",
    "            # Reshape output and target for loss calculation\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            tgt_y = tgt_y.view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, tgt_y)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "248f8239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 100\u001b[39m\n\u001b[32m     97\u001b[39m         \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     59\u001b[39m     train_loss = train_transformer(model, train_loader, optimizer, criterion, device)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     val_loss = \u001b[43mevaluate_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     train_losses.append(train_loss)\n\u001b[32m     63\u001b[39m     val_losses.append(val_loss)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mevaluate_transformer\u001b[39m\u001b[34m(model, val_loader, criterion, device)\u001b[39m\n\u001b[32m     46\u001b[39m tgt_mask = create_padding_mask(tgt) & create_look_ahead_mask(tgt.size(\u001b[32m1\u001b[39m)).to(device)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Reshape output and target for loss calculation\u001b[39;00m\n\u001b[32m     52\u001b[39m output = output.view(-\u001b[32m1\u001b[39m, output.size(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, src, tgt, src_mask, tgt_mask)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, src_mask=\u001b[38;5;28;01mNone\u001b[39;00m, tgt_mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Encode source sequence\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     enc_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Decode target sequence\u001b[39;00m\n\u001b[32m     14\u001b[39m     dec_output = \u001b[38;5;28mself\u001b[39m.decoder(tgt, enc_output, src_mask=src_mask, tgt_mask=tgt_mask)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mEncoder.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m# Apply embedding and positional encoding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m * (\u001b[38;5;28mself\u001b[39m.embedding.embedding_dim ** \u001b[32m0.5\u001b[39m)\n\u001b[32m     34\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.positional_encoding(x)\n\u001b[32m     35\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/Python/ML_from_scratch/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create dataset\n",
    "    eng_sentences, fr_sentences, src_vocab, tgt_vocab, src_idx2word, tgt_idx2word = create_toy_dataset()\n",
    "    \n",
    "    # Create train and validation datasets\n",
    "    train_size = int(0.8 * len(eng_sentences))\n",
    "    train_dataset = TranslationDataset(\n",
    "        eng_sentences[:train_size], \n",
    "        fr_sentences[:train_size], \n",
    "        src_vocab, \n",
    "        tgt_vocab\n",
    "    )\n",
    "    val_dataset = TranslationDataset(\n",
    "        eng_sentences[train_size:], \n",
    "        fr_sentences[train_size:], \n",
    "        src_vocab, \n",
    "        tgt_vocab\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 2\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "    \n",
    "    # Use smaller model for toy dataset\n",
    "    model = Transformer(\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        d_model=64,\n",
    "        num_heads=2,\n",
    "        d_ff=128,\n",
    "        num_layers=2,\n",
    "        dropout=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[''])\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_transformer(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss = evaluate_transformer(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.savefig('transformer_loss.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test translation\n",
    "    test_sentences = [\n",
    "        'hello how are you',\n",
    "        'i love programming',\n",
    "        'thank you very much'\n",
    "    ]\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
    "    \n",
    "    print(\"\\nTest Translations:\")\n",
    "    for sentence in test_sentences:\n",
    "        translation = translate(model, sentence, src_vocab, tgt_idx2word, device)\n",
    "        print(f\"English: {sentence}\")\n",
    "        print(f\"French: {translation}\")\n",
    "        print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
